{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df44b929-c205-46ef-8c51-501bbc26c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "import branca.colormap as cm\n",
    "import re\n",
    "from branca.colormap import linear\n",
    "\n",
    "#Extract the inflows and the outflows of each state from the flow data using rootsid as id \n",
    "# Specify the directory containing the CSV files\n",
    "directory_path = 'flows'\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for file_name in os.listdir(directory_path):\n",
    "    try:\n",
    "        # Check if the file is a CSV\n",
    "        if file_name.endswith('.csv'):\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "\n",
    "            # Read the CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Get unique Ids \n",
    "            states = set(df['origState']).union(set(df['destState']))\n",
    "\n",
    "            # Calculate outflows volumes from familycnt column \n",
    "            outflows_df = df.groupby('origState')['familycnt'].sum().reset_index()\n",
    "            outflows_df.rename(columns={'origState': 'rootsid', 'familycnt': 'Outflows'}, inplace=True)\n",
    "\n",
    "            # Calculate inflows volumes from familycnt column \n",
    "            inflows_df = df.groupby('destState')['familycnt'].sum().reset_index()\n",
    "            inflows_df.rename(columns={'destState': 'rootsid', 'familycnt': 'Inflows'}, inplace=True)\n",
    "\n",
    "            # Create a DataFrame with all State IDs\n",
    "            states_df = pd.DataFrame({'rootsid': list(states)})\n",
    "\n",
    "            # Merge all States with outflows and inflows DataFrames\n",
    "            merged_df = pd.merge(states_df, outflows_df, on='rootsid', how='left')\n",
    "            merged_df = pd.merge(merged_df, inflows_df, on='rootsid', how='left')\n",
    "\n",
    "            # Fill NaN values with 0 for Outflows and Inflows\n",
    "            merged_df['Outflows'] = merged_df['Outflows'].fillna(0)\n",
    "            merged_df['Inflows'] = merged_df['Inflows'].fillna(0)\n",
    "\n",
    "            # Filter out State IDs not between 101 and 158\n",
    "            merged_df = merged_df[(merged_df['rootsid'] >= 101) & (merged_df['rootsid'] <= 158)]\n",
    "\n",
    "            # Sort the DataFrame by State ID in ascending order\n",
    "            merged_df.sort_values(by='rootsid', inplace=True)\n",
    "\n",
    "            # Reset the index after sorting\n",
    "            merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            # Construct the output file path\n",
    "            output_file_path = os.path.join(directory_path, f'processed_{file_name}')\n",
    "\n",
    "            # Save the results to a new CSV file\n",
    "            merged_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "            print(f\"Processed data saved to {output_file_path}\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"No data in file {file_name}, skipped.\")\n",
    "    except pd.errors.ParserError:\n",
    "        print(f\"Error parsing file {file_name}, skipped.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred processing {file_name}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce642ae-75ff-4049-908d-393b7739ee21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the output of the previous code, perform simple averages to smoothen the data. This uses a 5 year window average to interpolate values \n",
    "directory_path = 'flows'\n",
    "\n",
    "# Define the window size for the moving average\n",
    "window_size = 5\n",
    "\n",
    "# Define the full range of rootsid values\n",
    "all_rootsid = range(101, 159)  # 101 to 158 inclusive\n",
    "\n",
    "# Loop through each processed file in the directory\n",
    "for file_name in os.listdir(directory_path):\n",
    "    try:\n",
    "        if file_name.startswith('processed_') and file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "\n",
    "            # Read the CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Ensure all rootsid are present by creating a template DataFrame with all rootsid values\n",
    "            # and merging it with the existing data, filling missing values explicitly with 0\n",
    "            template_df = pd.DataFrame({'rootsid': all_rootsid})\n",
    "            df = pd.merge(template_df, df, on='rootsid', how='left').fillna(0)  # Fill all NaNs with 0\n",
    "\n",
    "            # Calculate the moving averages for Outflows and Inflows\n",
    "            df['MA_Outflows'] = np.ceil(df['Outflows'].rolling(window=window_size, min_periods=1).mean())\n",
    "            df['MA_Inflows'] = np.ceil(df['Inflows'].rolling(window=window_size, min_periods=1).mean())\n",
    "\n",
    "            # Save the results to a new CSV file, ensuring not to overwrite the original processed files\n",
    "            output_file_path = os.path.join(directory_path, f'avg_{file_name}')\n",
    "            df.to_csv(output_file_path, index=False)\n",
    "\n",
    "            print(f\"Moving averages calculated and saved to {output_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {file_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573ea1a0-66d3-4de0-9839-791a51fdeafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets the year from the name of the file and creates a year field with this data and calculate the netflow ratio (Migration efficiency).\n",
    "# Define the directory path that contains the processed files\n",
    "directory_path = 'flows'\n",
    "\n",
    "# Loop through each processed file in the directory\n",
    "for file_name in os.listdir(directory_path):\n",
    "    try:\n",
    "        if file_name.startswith('avg_processed_') and file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "\n",
    "            # Extract the year from the file name using regular expression\n",
    "            # This regex captures four digits at the end of the filename just before '.csv'\n",
    "            match = re.search(r'_(\\d{4}).csv$', file_name)\n",
    "            if match:\n",
    "                year = int(match.group(1))  # Convert the extracted year to an integer\n",
    "            else:\n",
    "                print(f\"No year found in the file name {file_name}. Skipping this file.\")\n",
    "                continue  # Skip files where no year is found\n",
    "\n",
    "            # Read the CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Add a new column 'year' to the DataFrame\n",
    "            df['year'] = year\n",
    "\n",
    "            # Calculate the netflow ratio\n",
    "            df['Netflow_Ratio'] = (df['MA_Inflows'] - df['MA_Outflows']) / (df['MA_Inflows'] + df['MA_Outflows'] + 1e-10)\n",
    "            df['Netflow_Ratio'] = df['Netflow_Ratio'].round(2)\n",
    "\n",
    "            # Save the results to a new CSV file\n",
    "            output_file_path = os.path.join(directory_path, f'Net_{file_name}')\n",
    "            df.to_csv(output_file_path, index=False)\n",
    "\n",
    "            print(f\"Netflow Ratio calculated and saved to {output_file_path} with the year {year}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {file_name}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07eac309-c226-456d-96d7-8f1a75c33601",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine the output of the previous code to one single dataframe or csv\n",
    "# Define the directory path that contains the processed files\n",
    "directory_path = 'flows'\n",
    "\n",
    "# Initialize an empty DataFrame to store all combined data\n",
    "all_data = pd.DataFrame()\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for file_name in os.listdir(directory_path):\n",
    "    # Check if the file is one of the output netflow ratio files\n",
    "    if file_name.startswith('Net_avg_processed_') and file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Append the dataframe to the all_data DataFrame\n",
    "        all_data = pd.concat([all_data, df], ignore_index=True)\n",
    "\n",
    "# Save the combined data to a new CSV file\n",
    "combined_output_path = os.path.join(directory_path, 'migration.csv')\n",
    "all_data.to_csv(combined_output_path, index=False)\n",
    "\n",
    "print(f\"All data combined and saved to {combined_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13abb58e-00cd-408a-9bf4-eed677374660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the state names from the geojson and populated the migration data with the correponding name, join through the rootsid \n",
    "# Load both CSV files\n",
    "df1 = pd.read_csv('flows\\migration.csv')\n",
    "df2 = pd.read_csv('statename.csv')\n",
    "\n",
    "# Create a dictionary to map rootsid to name from the second dataframe\n",
    "name_map = pd.Series(df2.name.values, index=df2.rootsid).to_dict()\n",
    "\n",
    "# Map the names to the first dataframe using the map\n",
    "df1['name'] = df1['rootsid'].map(name_map)\n",
    "\n",
    "# Save the updated dataframe to a new CSV file\n",
    "df1.to_csv('Migration.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ce1a27-201d-4fb5-9cb3-06d6dd3fb253",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculates the total inflows and outflows over time, plot a trend map to identify year with highest inflow and outflow \n",
    "# Print the current working directory\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Print the current working directory\n",
    "print(\"Current Working Directory:\", current_directory)\n",
    "\n",
    "# Load CSV data into a DataFrame\n",
    "df = pd.read_csv('Migration.csv')\n",
    "\n",
    "# Group data by year and sum the inflows\n",
    "total_inflows = df.groupby('year')['MA_Inflows'].sum().reset_index()\n",
    "# Group data by year and sum the Outflows\n",
    "total_outflows = df.groupby('year')['MA_Outflows'].sum().reset_index()\n",
    "\n",
    "# Plot the total outflows over time\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(total_outflows['year'], total_outflows['MA_Outflows'], marker='o', linestyle='-')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Total Outflows')\n",
    "plt.title('Total Outflows Over Time')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot the total inflows over time\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(total_inflows['year'], total_inflows['MA_Inflows'], marker='o', linestyle='-')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Total Inflows')\n",
    "plt.title('Total Inflows Over Time')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee7d07e-f49f-471a-834d-5c427923dbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arranges the state by the highest inflows and outflows 1800-1900\n",
    "# Load CSV data into a DataFrame\n",
    "df = pd.read_csv('Migration.csv')\n",
    "\n",
    "# Aggregate data at the state level and sum the inflows and outflows\n",
    "state_level_data = df.groupby('name')[['MA_Inflows', 'MA_Outflows']].sum().reset_index()\n",
    "\n",
    "# Find the top 5 states with the highest inflows\n",
    "top_inflow_states = state_level_data.nlargest(5, 'MA_Inflows')\n",
    "\n",
    "# Find the top 5 states with the highest outflows\n",
    "top_outflow_states = state_level_data.nlargest(5, 'MA_Outflows')\n",
    "\n",
    "print(\"Top 5 states with the highest inflows:\")\n",
    "for index, state in top_inflow_states.iterrows():\n",
    "    print(f\"State: {state['name']}, Inflows: {state['MA_Inflows']}\")\n",
    "\n",
    "print(\"\\nTop 5 states with the highest outflows:\")\n",
    "for index, state in top_outflow_states.iterrows():\n",
    "    print(f\"State: {state['name']}, Outflows: {state['MA_Outflows']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c794fe2-0afb-4a26-86cd-363eb158b9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics of data\n",
    "print(\"Mean Inflows:\", df['MA_Inflows'].mean())\n",
    "print(\"Median Inflows:\", df['MA_Inflows'].median())\n",
    "print(\"Inflow Variance:\", df['MA_Inflows'].var())\n",
    "print(\"Inflow Standard Deviation:\", df['MA_Inflows'].std())\n",
    "\n",
    "\n",
    "print(\"Mean Outflows:\", df['MA_Outflows'].mean())\n",
    "print(\"Median Outflows:\", df['MA_Outflows'].median())\n",
    "print(\"Outflow Variance:\", df['MA_Outflows'].var())\n",
    "print(\"Outflow Standard Deviation:\", df['MA_Outflows'].std())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f597ee3f-2cb0-4e2c-97c2-dca0135b87ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#proportional  maps of outflow\n",
    "# Load the shapefile into a GeoDataFrame\n",
    "shapefile_path = 'America.geojson'\n",
    "geo_df = gpd.read_file(shapefile_path)\n",
    "\n",
    "# Load additional data from a CSV file\n",
    "csv_path =  'Migration.csv'\n",
    "data_df = pd.read_csv(csv_path)\n",
    "\n",
    "#Sum 'Outflows' for each region in the CSV data if needed\n",
    "summed_data = data_df.groupby('rootsid')['MA_Outflows'].sum().reset_index()\n",
    "\n",
    "# Merge the GeoDataFrame with the summed data DataFrame using a id (rootsid) key\n",
    "merged_gdf = geo_df.merge(summed_data, on='rootsid')\n",
    "\n",
    "# Calculate centroids for plotting proportional symbols\n",
    "merged_gdf['centroids'] = merged_gdf.geometry.centroid\n",
    "\n",
    "# Set up the plot\n",
    "fig, ax = plt.subplots(figsize=(30, 20))\n",
    "merged_gdf.plot(ax=ax, color='#f0f0f0', edgecolor='black', linewidth=0.5)  # Plot polygons\n",
    "\n",
    "# Check the max outflows to adjust the size factor dynamically\n",
    "max_outflow = merged_gdf['MA_Outflows'].max()\n",
    "size_factor = 5000 / max_outflow  # Adjust this factor based on visual preferences and the data range\n",
    "\n",
    "# Plot proportional symbols using 'outflows' summed values for the size\n",
    "for idx, row in merged_gdf.iterrows():\n",
    "    symbol_color = '#636363' \n",
    "    ax.scatter(row['centroids'].x, row['centroids'].y, s=row['MA_Outflows'] * size_factor, color=symbol_color, alpha=0.6)\n",
    "\n",
    "# Plot proportional symbols\n",
    "size_factor = 0.00005  # Adjust the size_factor to scale symbol sizes appropriately\n",
    "markersize = merged_gdf['MA_Outflows'] * size_factor\n",
    "merged_gdf.plot(ax=ax, marker='o', color='black', markersize=markersize, alpha=0.6)\n",
    "\n",
    "\n",
    "# Add a nested legend\n",
    "legend_sizes = [1000000, 10000000, 50000000]  # Custom ranges: <1M, 1M-10M, 10M-50M\n",
    "legend_labels = ['< 1M', '1M - 10M', '10M - 50M']\n",
    "legend_elements = [plt.Line2D([0], [0], marker='o', color='w', label=label,\n",
    "                              markerfacecolor='grey', markersize=np.sqrt(size * size_factor / np.pi)) for size, label in zip(legend_sizes, legend_labels)]\n",
    "ax.legend(handles=legend_elements, title='Total Outflows', loc='upper left',\n",
    "          bbox_to_anchor=(1, 1), fontsize='medium', frameon=True, shadow=True, borderpad=2.0,\n",
    "          labelspacing=3.5)\n",
    "\n",
    "# Set title\n",
    "ax.set_title(\"Proportional Symbol Map of Outflows\", fontsize=20)\n",
    "\n",
    "# Remove axis\n",
    "ax.set_axis_off()\n",
    "\n",
    "plt.show()  #  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46864277-c70c-4456-aac8-a1b30269febc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#poroportional maps of inflows\n",
    "# Load the shapefile into a GeoDataFrame\n",
    "shapefile_path = 'America.geojson'\n",
    "geo_df = gpd.read_file(shapefile_path)\n",
    "\n",
    "# Load additional data from a CSV file\n",
    "csv_path = 'Migration.csv'\n",
    "data_df = pd.read_csv(csv_path)\n",
    "\n",
    "#Sum 'Outflows' for each region in the CSV data if needed\n",
    "summed_data = data_df.groupby('rootsid')['MA_Inflows'].sum().reset_index()\n",
    "\n",
    "# Merge the GeoDataFrame with the summed data DataFrame using a common key\n",
    "# Ensure both geo_df and data_df contain a 'region_key' column that you merge on\n",
    "merged_gdf = geo_df.merge(summed_data, on='rootsid')\n",
    "\n",
    "# Calculate centroids for plotting proportional symbols\n",
    "merged_gdf['centroids'] = merged_gdf.geometry.centroid\n",
    "\n",
    "# Set up the plot\n",
    "fig, ax = plt.subplots(figsize=(30, 20))\n",
    "merged_gdf.plot(ax=ax, color='white', edgecolor='black', linewidth=0.5)\n",
    "\n",
    "# Check the max inflow to adjust the size factor dynamically\n",
    "max_inflow = merged_gdf['MA_Inflows'].max()\n",
    "size_factor = 5000 / max_outflow  # Adjust this factor based on visual preferences and the data range\n",
    "\n",
    "# Plot proportional symbols using 'outflows' summed values for the size\n",
    "for idx, row in merged_gdf.iterrows():\n",
    "    symbol_color = '#636363'  # Default color, replace or modify as needed\n",
    "    ax.scatter(row['centroids'].x, row['centroids'].y, s=row['MA_Inflows'] * size_factor, color=symbol_color, alpha=0.6)\n",
    "\n",
    "# Plot proportional symbols\n",
    "size_factor = 0.00005  # Adjust the size_factor to scale symbol sizes appropriately\n",
    "markersize = merged_gdf['MA_Inflows'] * size_factor\n",
    "merged_gdf.plot(ax=ax, marker='o', color='black', markersize=markersize, alpha=0.6)\n",
    "\n",
    "\n",
    "# Add a nested legend\n",
    "legend_sizes = [1000000, 10000000, 50000000]  # Custom ranges: <1M, 1M-10M, 10M-50M\n",
    "legend_labels = ['< 1M', '1M - 10M', '10M - 50M']\n",
    "legend_elements = [plt.Line2D([0], [0], marker='o', color='w', label=label,\n",
    "                              markerfacecolor='grey', markersize=np.sqrt(size * size_factor / np.pi)) for size, label in zip(legend_sizes, legend_labels)]\n",
    "ax.legend(handles=legend_elements, title='Total Inflows', loc='upper left',\n",
    "          bbox_to_anchor=(1, 1), fontsize='medium', frameon=True, shadow=True, borderpad=2.0,\n",
    "          labelspacing=3.5)\n",
    "\n",
    "# Set title\n",
    "ax.set_title(\"Proportional Symbol Map of Inflows\", fontsize=20)\n",
    "\n",
    "# Remove axis\n",
    "ax.set_axis_off()\n",
    "\n",
    "plt.show()  # Display the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e6023e-8c37-48fc-baf4-a22c48cb2023",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Paths to the data\n",
    "shapefile_path = 'America.geojson'\n",
    "csv_path = 'Migration.csv'\n",
    "\n",
    "try:\n",
    "    # Load GeoJSON and CSV data\n",
    "    geo_df = gpd.read_file(shapefile_path)\n",
    "    data_df = pd.read_csv(csv_path)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    raise SystemExit(\"Failed to load shapefiles or CSV.\")\n",
    "\n",
    "try:\n",
    "    # Convert year to string and filter rows\n",
    "    data_df['year'] = data_df['year'].astype(str)\n",
    "    data_df = data_df[data_df['year'] >= '1800']\n",
    "\n",
    "    # Merge GeoDataFrame with DataFrame\n",
    "    merged_gdf = geo_df.merge(data_df, on='rootsid', how='left')\n",
    "\n",
    "    # Calculate quantiles\n",
    "    quantile_values = merged_gdf['Netflow_Ratio'].dropna().quantile([0, 0.2, 0.4, 0.6, 0.8, 1.0]).values\n",
    "    colormap = linear.RdYlBu_11.scale(quantile_values.min(), quantile_values.max()).to_step(n=len(quantile_values) - 1)\n",
    "    colormap.caption = 'Netflow_Ratio'\n",
    "except Exception as e:\n",
    "    print(f\"Error processing data: {e}\")\n",
    "    raise SystemExit(\"Failed during data processing.\")\n",
    "\n",
    "try:\n",
    "    # Create map\n",
    "    latitude = 39.8283\n",
    "    longitude = -98.5795\n",
    "    m = folium.Map(location=[latitude, longitude], zoom_start=5)\n",
    "\n",
    "    # Define style function using quantile breaks\n",
    "    def style_function(feature):\n",
    "        Netflow_Ratio = feature['properties']['Netflow_Ratio']\n",
    "        return {\n",
    "            'fillColor': colormap(Netflow_Ratio) if Netflow_Ratio is not None else 'transparent',\n",
    "            'color': 'black',\n",
    "            'weight': 0.5,\n",
    "            'fillOpacity': 0.7\n",
    "        }\n",
    "\n",
    "    # Add layers for each year\n",
    "    for year in sorted(data_df['year'].unique()):\n",
    "        year_data = merged_gdf[merged_gdf['year'] == year]\n",
    "        fg = folium.FeatureGroup(name=f'Year {year}', show=False)\n",
    "        folium.GeoJson(\n",
    "            data=year_data,\n",
    "            style_function=style_function,\n",
    "            tooltip=folium.GeoJsonTooltip(\n",
    "                fields=['rootsid', 'Netflow_Ratio'],\n",
    "                aliases=['Roots ID:', 'Netflow_Ratio:'],\n",
    "                localize=True\n",
    "            )\n",
    "        ).add_to(fg)\n",
    "        fg.add_to(m)\n",
    "\n",
    "    # Add LayerControl and colormap\n",
    "    folium.LayerControl().add_to(m)\n",
    "    colormap.add_to(m)\n",
    "\n",
    "    # Save or display map\n",
    "    m.save('migration_quantiles.html')\n",
    "except Exception as e:\n",
    "    print(f\"Error creating map: {e}\")\n",
    "    raise SystemExit(\"Failed during map creation.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
